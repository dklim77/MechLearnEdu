{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "FinalTask_4Team_Rev2.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIErL4bB3Cfq"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AG7I7NEe3Cfs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c37939c6-477c-449b-8a7b-4e8ae1cabf06"
      },
      "source": [
        "#data = pd.read_excel(\"D:\\MAGVI_500RT_RawData.xlsx\")\n",
        "url = \"https://github.com/dklim77/MechLearnEdu/blob/master/MAGVI_500RT_RawData.xlsx?raw=true\"\n",
        "data = pd.read_excel(url)\n",
        "data.info()"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1524 entries, 0 to 1523\n",
            "Data columns (total 17 columns):\n",
            " #   Column              Non-Null Count  Dtype  \n",
            "---  ------              --------------  -----  \n",
            " 0   No                  1524 non-null   int64  \n",
            " 1   EvaporatingTemp     1524 non-null   float64\n",
            " 2   CondensingTemp      1524 non-null   float64\n",
            " 3   IGVOpening          1524 non-null   int64  \n",
            " 4   HotGasFraction      1524 non-null   int64  \n",
            " 5   RPM                 1524 non-null   int64  \n",
            " 6   Capacity            1524 non-null   float64\n",
            " 7   InputPower          1524 non-null   float64\n",
            " 8   FirstTopHead        1524 non-null   float64\n",
            " 9   SecondTopHead       1524 non-null   float64\n",
            " 10  1stCompHead         1524 non-null   float64\n",
            " 11  2ndCompHead         1524 non-null   float64\n",
            " 12  TotalHead           1524 non-null   float64\n",
            " 13  1stCompSurgeMargin  1524 non-null   float64\n",
            " 14  2ndCompSurgeMargin  1524 non-null   float64\n",
            " 15  VFR_1st             1524 non-null   float64\n",
            " 16  VFR_2nd             1524 non-null   float64\n",
            "dtypes: float64(13), int64(4)\n",
            "memory usage: 202.5 KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XG2Z909k3Cft",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "outputId": "f8674c45-bc11-4d05-d44f-f9c826054ae7"
      },
      "source": [
        "print(data.shape)\n",
        "data[:5]"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1524, 17)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>No</th>\n",
              "      <th>EvaporatingTemp</th>\n",
              "      <th>CondensingTemp</th>\n",
              "      <th>IGVOpening</th>\n",
              "      <th>HotGasFraction</th>\n",
              "      <th>RPM</th>\n",
              "      <th>Capacity</th>\n",
              "      <th>InputPower</th>\n",
              "      <th>FirstTopHead</th>\n",
              "      <th>SecondTopHead</th>\n",
              "      <th>1stCompHead</th>\n",
              "      <th>2ndCompHead</th>\n",
              "      <th>TotalHead</th>\n",
              "      <th>1stCompSurgeMargin</th>\n",
              "      <th>2ndCompSurgeMargin</th>\n",
              "      <th>VFR_1st</th>\n",
              "      <th>VFR_2nd</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>2.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>80</td>\n",
              "      <td>0</td>\n",
              "      <td>15001</td>\n",
              "      <td>543.8</td>\n",
              "      <td>274.61</td>\n",
              "      <td>1664.8</td>\n",
              "      <td>1155.8</td>\n",
              "      <td>1072.7</td>\n",
              "      <td>136.3</td>\n",
              "      <td>1209.0</td>\n",
              "      <td>592.1</td>\n",
              "      <td>1019.5</td>\n",
              "      <td>0.7060</td>\n",
              "      <td>0.4532</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>2.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>60</td>\n",
              "      <td>0</td>\n",
              "      <td>15001</td>\n",
              "      <td>479.5</td>\n",
              "      <td>224.12</td>\n",
              "      <td>1585.0</td>\n",
              "      <td>1100.4</td>\n",
              "      <td>1016.9</td>\n",
              "      <td>192.4</td>\n",
              "      <td>1209.4</td>\n",
              "      <td>568.1</td>\n",
              "      <td>907.9</td>\n",
              "      <td>0.6183</td>\n",
              "      <td>0.4088</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>2.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>40</td>\n",
              "      <td>0</td>\n",
              "      <td>15001</td>\n",
              "      <td>389.1</td>\n",
              "      <td>186.39</td>\n",
              "      <td>1550.7</td>\n",
              "      <td>1077.4</td>\n",
              "      <td>979.7</td>\n",
              "      <td>234.1</td>\n",
              "      <td>1213.8</td>\n",
              "      <td>571.0</td>\n",
              "      <td>843.3</td>\n",
              "      <td>0.4995</td>\n",
              "      <td>0.3392</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5</td>\n",
              "      <td>2.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>15001</td>\n",
              "      <td>254.7</td>\n",
              "      <td>146.55</td>\n",
              "      <td>1512.2</td>\n",
              "      <td>1053.8</td>\n",
              "      <td>931.7</td>\n",
              "      <td>300.5</td>\n",
              "      <td>1232.1</td>\n",
              "      <td>580.5</td>\n",
              "      <td>753.4</td>\n",
              "      <td>0.3253</td>\n",
              "      <td>0.2330</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6</td>\n",
              "      <td>2.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>15001</td>\n",
              "      <td>58.3</td>\n",
              "      <td>94.05</td>\n",
              "      <td>1095.8</td>\n",
              "      <td>771.2</td>\n",
              "      <td>1060.0</td>\n",
              "      <td>292.3</td>\n",
              "      <td>1352.3</td>\n",
              "      <td>35.8</td>\n",
              "      <td>478.9</td>\n",
              "      <td>0.0760</td>\n",
              "      <td>0.0606</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   No  EvaporatingTemp  CondensingTemp  ...  2ndCompSurgeMargin  VFR_1st  VFR_2nd\n",
              "0   2              2.0            19.0  ...              1019.5   0.7060   0.4532\n",
              "1   3              2.0            19.0  ...               907.9   0.6183   0.4088\n",
              "2   4              2.0            19.0  ...               843.3   0.4995   0.3392\n",
              "3   5              2.0            19.0  ...               753.4   0.3253   0.2330\n",
              "4   6              2.0            19.0  ...               478.9   0.0760   0.0606\n",
              "\n",
              "[5 rows x 17 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNX4LoyP3Cfu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "outputId": "c6118695-8bdf-42d5-a212-24a67421a5b5"
      },
      "source": [
        "# 사본 사용 (원본 백업)\n",
        "df = data.copy()\n",
        "df[:3]"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>No</th>\n",
              "      <th>EvaporatingTemp</th>\n",
              "      <th>CondensingTemp</th>\n",
              "      <th>IGVOpening</th>\n",
              "      <th>HotGasFraction</th>\n",
              "      <th>RPM</th>\n",
              "      <th>Capacity</th>\n",
              "      <th>InputPower</th>\n",
              "      <th>FirstTopHead</th>\n",
              "      <th>SecondTopHead</th>\n",
              "      <th>1stCompHead</th>\n",
              "      <th>2ndCompHead</th>\n",
              "      <th>TotalHead</th>\n",
              "      <th>1stCompSurgeMargin</th>\n",
              "      <th>2ndCompSurgeMargin</th>\n",
              "      <th>VFR_1st</th>\n",
              "      <th>VFR_2nd</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>2.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>80</td>\n",
              "      <td>0</td>\n",
              "      <td>15001</td>\n",
              "      <td>543.8</td>\n",
              "      <td>274.61</td>\n",
              "      <td>1664.8</td>\n",
              "      <td>1155.8</td>\n",
              "      <td>1072.7</td>\n",
              "      <td>136.3</td>\n",
              "      <td>1209.0</td>\n",
              "      <td>592.1</td>\n",
              "      <td>1019.5</td>\n",
              "      <td>0.7060</td>\n",
              "      <td>0.4532</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>2.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>60</td>\n",
              "      <td>0</td>\n",
              "      <td>15001</td>\n",
              "      <td>479.5</td>\n",
              "      <td>224.12</td>\n",
              "      <td>1585.0</td>\n",
              "      <td>1100.4</td>\n",
              "      <td>1016.9</td>\n",
              "      <td>192.4</td>\n",
              "      <td>1209.4</td>\n",
              "      <td>568.1</td>\n",
              "      <td>907.9</td>\n",
              "      <td>0.6183</td>\n",
              "      <td>0.4088</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>2.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>40</td>\n",
              "      <td>0</td>\n",
              "      <td>15001</td>\n",
              "      <td>389.1</td>\n",
              "      <td>186.39</td>\n",
              "      <td>1550.7</td>\n",
              "      <td>1077.4</td>\n",
              "      <td>979.7</td>\n",
              "      <td>234.1</td>\n",
              "      <td>1213.8</td>\n",
              "      <td>571.0</td>\n",
              "      <td>843.3</td>\n",
              "      <td>0.4995</td>\n",
              "      <td>0.3392</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   No  EvaporatingTemp  CondensingTemp  ...  2ndCompSurgeMargin  VFR_1st  VFR_2nd\n",
              "0   2              2.0            19.0  ...              1019.5   0.7060   0.4532\n",
              "1   3              2.0            19.0  ...               907.9   0.6183   0.4088\n",
              "2   4              2.0            19.0  ...               843.3   0.4995   0.3392\n",
              "\n",
              "[3 rows x 17 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCxNkrvb3Cfu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "outputId": "ad1a3ff3-431e-4261-f66d-831af72f1ee2"
      },
      "source": [
        "# 스케일링\n",
        "\n",
        "# 표준 스케일링\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scale = StandardScaler()\n",
        "df[[\"EvaporatingTemp_sc\",\"CondensingTemp_sc\",\"IGVOpening_sc\",\"RPM_sc\"]] = scale.fit_transform(df[[\"EvaporatingTemp\",\"CondensingTemp\",\"IGVOpening\",\"RPM\"]])\n",
        "\n",
        "# Min Max 스케일링\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "minmax = MinMaxScaler()\n",
        "df[[\"EvaporatingTemp_mmsc\",\"CondensingTemp_mmsc\",\"IGVOpening_mmsc\",\"RPM_mmsc\"]] = minmax.fit_transform(df[[\"EvaporatingTemp\",\"CondensingTemp\",\"IGVOpening\",\"RPM\"]])\n",
        "\n",
        "df[:3]"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>No</th>\n",
              "      <th>EvaporatingTemp</th>\n",
              "      <th>CondensingTemp</th>\n",
              "      <th>IGVOpening</th>\n",
              "      <th>HotGasFraction</th>\n",
              "      <th>RPM</th>\n",
              "      <th>Capacity</th>\n",
              "      <th>InputPower</th>\n",
              "      <th>FirstTopHead</th>\n",
              "      <th>SecondTopHead</th>\n",
              "      <th>1stCompHead</th>\n",
              "      <th>2ndCompHead</th>\n",
              "      <th>TotalHead</th>\n",
              "      <th>1stCompSurgeMargin</th>\n",
              "      <th>2ndCompSurgeMargin</th>\n",
              "      <th>VFR_1st</th>\n",
              "      <th>VFR_2nd</th>\n",
              "      <th>EvaporatingTemp_sc</th>\n",
              "      <th>CondensingTemp_sc</th>\n",
              "      <th>IGVOpening_sc</th>\n",
              "      <th>RPM_sc</th>\n",
              "      <th>EvaporatingTemp_mmsc</th>\n",
              "      <th>CondensingTemp_mmsc</th>\n",
              "      <th>IGVOpening_mmsc</th>\n",
              "      <th>RPM_mmsc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>2.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>80</td>\n",
              "      <td>0</td>\n",
              "      <td>15001</td>\n",
              "      <td>543.8</td>\n",
              "      <td>274.61</td>\n",
              "      <td>1664.8</td>\n",
              "      <td>1155.8</td>\n",
              "      <td>1072.7</td>\n",
              "      <td>136.3</td>\n",
              "      <td>1209.0</td>\n",
              "      <td>592.1</td>\n",
              "      <td>1019.5</td>\n",
              "      <td>0.7060</td>\n",
              "      <td>0.4532</td>\n",
              "      <td>-1.622635</td>\n",
              "      <td>-1.391129</td>\n",
              "      <td>0.668366</td>\n",
              "      <td>1.095127</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>2.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>60</td>\n",
              "      <td>0</td>\n",
              "      <td>15001</td>\n",
              "      <td>479.5</td>\n",
              "      <td>224.12</td>\n",
              "      <td>1585.0</td>\n",
              "      <td>1100.4</td>\n",
              "      <td>1016.9</td>\n",
              "      <td>192.4</td>\n",
              "      <td>1209.4</td>\n",
              "      <td>568.1</td>\n",
              "      <td>907.9</td>\n",
              "      <td>0.6183</td>\n",
              "      <td>0.4088</td>\n",
              "      <td>-1.622635</td>\n",
              "      <td>-1.391129</td>\n",
              "      <td>-0.019406</td>\n",
              "      <td>1.095127</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.6</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>2.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>40</td>\n",
              "      <td>0</td>\n",
              "      <td>15001</td>\n",
              "      <td>389.1</td>\n",
              "      <td>186.39</td>\n",
              "      <td>1550.7</td>\n",
              "      <td>1077.4</td>\n",
              "      <td>979.7</td>\n",
              "      <td>234.1</td>\n",
              "      <td>1213.8</td>\n",
              "      <td>571.0</td>\n",
              "      <td>843.3</td>\n",
              "      <td>0.4995</td>\n",
              "      <td>0.3392</td>\n",
              "      <td>-1.622635</td>\n",
              "      <td>-1.391129</td>\n",
              "      <td>-0.707177</td>\n",
              "      <td>1.095127</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   No  EvaporatingTemp  ...  IGVOpening_mmsc  RPM_mmsc\n",
              "0   2              2.0  ...              0.8       1.0\n",
              "1   3              2.0  ...              0.6       1.0\n",
              "2   4              2.0  ...              0.4       1.0\n",
              "\n",
              "[3 rows x 25 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "mA3UWgcq3Cfv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82609a98-d70a-40ae-fb09-5c033899760a"
      },
      "source": [
        "#X = df[[\"EvaporatingTemp\", \"CondensingTemp\", \"IGVOpening\", \"RPM\"]]\n",
        "# 표준 스케일링 적용\n",
        "X = df[[\"EvaporatingTemp_sc\",\"CondensingTemp_sc\",\"IGVOpening_sc\",\"RPM_sc\"]]\n",
        "\n",
        "# Min Max 스케일링 적용\n",
        "#X = df[[\"EvaporatingTemp_mmsc\",\"CondensingTemp_mmsc\",\"IGVOpening_mmsc\",\"RPM_mmsc\"]]\n",
        "\n",
        "y = df[[\"Capacity\", \"InputPower\", \"FirstTopHead\", \"SecondTopHead\", \"1stCompHead\", \"2ndCompHead\"]]\n",
        "X[:5], y[:5]\n"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(   EvaporatingTemp_sc  CondensingTemp_sc  IGVOpening_sc    RPM_sc\n",
              " 0           -1.622635          -1.391129       0.668366  1.095127\n",
              " 1           -1.622635          -1.391129      -0.019406  1.095127\n",
              " 2           -1.622635          -1.391129      -0.707177  1.095127\n",
              " 3           -1.622635          -1.391129      -1.394948  1.095127\n",
              " 4           -1.622635          -1.391129      -2.082720  1.095127,\n",
              "    Capacity  InputPower  FirstTopHead  SecondTopHead  1stCompHead  2ndCompHead\n",
              " 0     543.8      274.61        1664.8         1155.8       1072.7        136.3\n",
              " 1     479.5      224.12        1585.0         1100.4       1016.9        192.4\n",
              " 2     389.1      186.39        1550.7         1077.4        979.7        234.1\n",
              " 3     254.7      146.55        1512.2         1053.8        931.7        300.5\n",
              " 4      58.3       94.05        1095.8          771.2       1060.0        292.3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Rsk67Nco3Cfv"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from keras import layers\n",
        "from keras import models\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import ModelCheckpoint"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SUt4zeG3Cfv"
      },
      "source": [
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
        "# monitor='val_loss' : validation set 의 loss 를 monitoring 한다\n",
        "# mode='min' : performance measure를 최소화 시킴, default는 'auto'\n",
        "# verbose=1 : 언제 keras 에서 training 을 멈추었는지를 화면에 출력\n",
        "\n",
        "# mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='min', save_best_only=True)\n",
        "# Early stopping 객체에 의해 트레이닝이 중지되었을 때 그 상태는 이전 모ㅔㄹ에 비해 일반적으로\n",
        "# validation error가 높은 상이일 것이다. 따라사 중지된 상태가 최고의 모델이 아닐 것이다.\n",
        "# 가장 validation performance가 좋은 모델을 저장하기 위해 keras에서는 ModelCheckpoint 객체가 존재"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7c7Wlpb3Cfw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b595e9a3-3b9d-42c9-8906-3539b3131d8a"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=9)    ################\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(70, activation='relu', input_shape = (4,)))    # 스케일링 할 것  x\n",
        "#model.add(layers.BatchNormalization())\n",
        "model.add(layers.Dense(70, activation='relu'))\n",
        "#model.add(layers.Dense(100, activation='relu'))\n",
        "# model.add(layers.Dense(100, activation='relu'))\n",
        "#model.add(layers.BatchNormalization())\n",
        "#model.add(layers.Dropout(0.2))\n",
        "model.add(layers.Dense(6, activation='linear'))    # activation='linear'  디폴트값임\n",
        "model.summary()\n",
        "# 안되면 레이어 추가할 것"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_12 (Dense)             (None, 70)                350       \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 70)                4970      \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 6)                 426       \n",
            "=================================================================\n",
            "Total params: 5,746\n",
            "Trainable params: 5,746\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WRP08eL3Cfw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "807727a9-f0fd-4e07-c21e-943109b96f5a"
      },
      "source": [
        "model.compile(optimizer= 'adam', loss = 'mean_squared_error')\n",
        "history = model.fit(X_train, y_train, epochs=300, batch_size=20, verbose=1, validation_split = 0.2, callbacks=[early_stopping])    # early stopping   5정도 적용  5~10"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "43/43 [==============================] - 1s 8ms/step - loss: 651958.0014 - val_loss: 652105.2500\n",
            "Epoch 2/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 664045.8878 - val_loss: 640646.8750\n",
            "Epoch 3/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 638047.4162 - val_loss: 606020.5000\n",
            "Epoch 4/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 590279.2543 - val_loss: 534320.8125\n",
            "Epoch 5/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 500549.2656 - val_loss: 424805.2812\n",
            "Epoch 6/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 390200.1286 - val_loss: 299929.4688\n",
            "Epoch 7/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 264621.9091 - val_loss: 189347.9844\n",
            "Epoch 8/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 161694.2209 - val_loss: 112614.9688\n",
            "Epoch 9/300\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 96455.7534 - val_loss: 68517.6875\n",
            "Epoch 10/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 59516.1314 - val_loss: 45821.2812\n",
            "Epoch 11/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 41890.2392 - val_loss: 34348.6250\n",
            "Epoch 12/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 31381.4238 - val_loss: 28508.8828\n",
            "Epoch 13/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 26542.3529 - val_loss: 25147.8340\n",
            "Epoch 14/300\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 25232.3519 - val_loss: 23129.7891\n",
            "Epoch 15/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 22828.0049 - val_loss: 21820.1680\n",
            "Epoch 16/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 20692.5169 - val_loss: 20929.4648\n",
            "Epoch 17/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 20254.8372 - val_loss: 20215.1387\n",
            "Epoch 18/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 19012.9456 - val_loss: 19693.2617\n",
            "Epoch 19/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 19257.4650 - val_loss: 19203.3906\n",
            "Epoch 20/300\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 17758.9205 - val_loss: 18674.7168\n",
            "Epoch 21/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 17555.5655 - val_loss: 18222.3379\n",
            "Epoch 22/300\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 17367.6763 - val_loss: 17673.1953\n",
            "Epoch 23/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 16452.1881 - val_loss: 17189.1934\n",
            "Epoch 24/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 17275.2810 - val_loss: 16699.9004\n",
            "Epoch 25/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 16009.3328 - val_loss: 16231.6484\n",
            "Epoch 26/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 15876.8292 - val_loss: 15772.2842\n",
            "Epoch 27/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 14675.7000 - val_loss: 15329.3438\n",
            "Epoch 28/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 14325.7538 - val_loss: 14894.1084\n",
            "Epoch 29/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 14556.8738 - val_loss: 14482.2344\n",
            "Epoch 30/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 13297.0075 - val_loss: 14098.6885\n",
            "Epoch 31/300\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 13190.6854 - val_loss: 13684.0752\n",
            "Epoch 32/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 12711.8903 - val_loss: 13249.8809\n",
            "Epoch 33/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 12530.9680 - val_loss: 12837.3848\n",
            "Epoch 34/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 11761.0252 - val_loss: 12446.3701\n",
            "Epoch 35/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 12336.4601 - val_loss: 12070.5713\n",
            "Epoch 36/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 12384.1867 - val_loss: 11699.0537\n",
            "Epoch 37/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 11005.0030 - val_loss: 11356.5303\n",
            "Epoch 38/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 10959.1056 - val_loss: 10992.9326\n",
            "Epoch 39/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 10339.4729 - val_loss: 10676.5371\n",
            "Epoch 40/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 10025.9471 - val_loss: 10331.2246\n",
            "Epoch 41/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 9474.3813 - val_loss: 10022.2939\n",
            "Epoch 42/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 9227.6502 - val_loss: 9703.4443\n",
            "Epoch 43/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 8897.3539 - val_loss: 9402.8740\n",
            "Epoch 44/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 8409.3629 - val_loss: 9086.9805\n",
            "Epoch 45/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 8052.9800 - val_loss: 8778.3584\n",
            "Epoch 46/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 8132.8922 - val_loss: 8476.8750\n",
            "Epoch 47/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 7531.5408 - val_loss: 8183.3091\n",
            "Epoch 48/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 7579.3344 - val_loss: 7869.1123\n",
            "Epoch 49/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 7481.9432 - val_loss: 7542.2852\n",
            "Epoch 50/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 6919.2577 - val_loss: 7242.3921\n",
            "Epoch 51/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 6443.4567 - val_loss: 6934.4863\n",
            "Epoch 52/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 6405.8102 - val_loss: 6603.2510\n",
            "Epoch 53/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 6359.0554 - val_loss: 6266.0815\n",
            "Epoch 54/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 5546.4119 - val_loss: 5943.7363\n",
            "Epoch 55/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 5642.4183 - val_loss: 5591.4238\n",
            "Epoch 56/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 5152.6987 - val_loss: 5247.6362\n",
            "Epoch 57/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 4943.1395 - val_loss: 4905.5171\n",
            "Epoch 58/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 4482.6011 - val_loss: 4571.8311\n",
            "Epoch 59/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 3887.2189 - val_loss: 4226.9399\n",
            "Epoch 60/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 3824.6194 - val_loss: 3868.6396\n",
            "Epoch 61/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 3363.1116 - val_loss: 3494.3967\n",
            "Epoch 62/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 2991.2847 - val_loss: 3155.3022\n",
            "Epoch 63/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 2927.2565 - val_loss: 2819.6116\n",
            "Epoch 64/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 2570.7119 - val_loss: 2545.0581\n",
            "Epoch 65/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 2311.9265 - val_loss: 2304.5591\n",
            "Epoch 66/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 2142.3686 - val_loss: 2111.0696\n",
            "Epoch 67/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 1875.1396 - val_loss: 1942.0396\n",
            "Epoch 68/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 1603.4611 - val_loss: 1796.5070\n",
            "Epoch 69/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 1561.5169 - val_loss: 1673.0940\n",
            "Epoch 70/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 1504.7308 - val_loss: 1583.0615\n",
            "Epoch 71/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 1592.8929 - val_loss: 1509.6909\n",
            "Epoch 72/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 1350.2110 - val_loss: 1455.0482\n",
            "Epoch 73/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 1297.5149 - val_loss: 1405.6331\n",
            "Epoch 74/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 1138.8778 - val_loss: 1367.0559\n",
            "Epoch 75/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 1314.8040 - val_loss: 1335.9807\n",
            "Epoch 76/300\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 1301.2223 - val_loss: 1307.0208\n",
            "Epoch 77/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 1343.4145 - val_loss: 1287.3777\n",
            "Epoch 78/300\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 1162.7063 - val_loss: 1262.6019\n",
            "Epoch 79/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 1019.2204 - val_loss: 1248.9457\n",
            "Epoch 80/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 1160.4733 - val_loss: 1226.6454\n",
            "Epoch 81/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 1052.4200 - val_loss: 1211.2743\n",
            "Epoch 82/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 1120.9102 - val_loss: 1198.3463\n",
            "Epoch 83/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 1119.9425 - val_loss: 1186.1383\n",
            "Epoch 84/300\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 1100.0422 - val_loss: 1169.5780\n",
            "Epoch 85/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 938.5422 - val_loss: 1155.2762\n",
            "Epoch 86/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 1027.9404 - val_loss: 1142.9697\n",
            "Epoch 87/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 1152.6412 - val_loss: 1133.1624\n",
            "Epoch 88/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 1005.0437 - val_loss: 1124.4902\n",
            "Epoch 89/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 862.3064 - val_loss: 1117.0669\n",
            "Epoch 90/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 1048.3048 - val_loss: 1097.1466\n",
            "Epoch 91/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 1086.0426 - val_loss: 1096.3154\n",
            "Epoch 92/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 1013.9134 - val_loss: 1082.6580\n",
            "Epoch 93/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 1061.7351 - val_loss: 1083.3057\n",
            "Epoch 94/300\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 1037.3647 - val_loss: 1066.4340\n",
            "Epoch 95/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 852.4055 - val_loss: 1061.8196\n",
            "Epoch 96/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 1084.9469 - val_loss: 1052.6178\n",
            "Epoch 97/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 833.5702 - val_loss: 1051.6215\n",
            "Epoch 98/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 932.4038 - val_loss: 1039.9681\n",
            "Epoch 99/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 869.3105 - val_loss: 1035.7463\n",
            "Epoch 100/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 991.2971 - val_loss: 1025.1195\n",
            "Epoch 101/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 846.3762 - val_loss: 1030.4303\n",
            "Epoch 102/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 983.8260 - val_loss: 1010.3616\n",
            "Epoch 103/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 848.8312 - val_loss: 1016.4650\n",
            "Epoch 104/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 779.4082 - val_loss: 1010.6935\n",
            "Epoch 105/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 837.7116 - val_loss: 994.5382\n",
            "Epoch 106/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 975.2477 - val_loss: 991.3628\n",
            "Epoch 107/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 807.7064 - val_loss: 985.9053\n",
            "Epoch 108/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 771.6820 - val_loss: 987.9474\n",
            "Epoch 109/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 740.3673 - val_loss: 983.7838\n",
            "Epoch 110/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 1007.9747 - val_loss: 970.6893\n",
            "Epoch 111/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 847.6614 - val_loss: 965.8300\n",
            "Epoch 112/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 1008.1824 - val_loss: 960.6809\n",
            "Epoch 113/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 893.7918 - val_loss: 961.7604\n",
            "Epoch 114/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 862.6866 - val_loss: 978.4322\n",
            "Epoch 115/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 969.1831 - val_loss: 950.4973\n",
            "Epoch 116/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 812.3032 - val_loss: 943.7868\n",
            "Epoch 117/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 1010.3065 - val_loss: 935.8690\n",
            "Epoch 118/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 998.6697 - val_loss: 936.8243\n",
            "Epoch 119/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 958.6028 - val_loss: 932.8285\n",
            "Epoch 120/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 784.0287 - val_loss: 931.7188\n",
            "Epoch 121/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 760.5094 - val_loss: 935.6558\n",
            "Epoch 122/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 858.2187 - val_loss: 924.7498\n",
            "Epoch 123/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 854.4240 - val_loss: 924.4052\n",
            "Epoch 124/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 801.3675 - val_loss: 902.6414\n",
            "Epoch 125/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 882.3366 - val_loss: 895.9526\n",
            "Epoch 126/300\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 807.6343 - val_loss: 891.7252\n",
            "Epoch 127/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 885.3225 - val_loss: 887.2689\n",
            "Epoch 128/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 774.0237 - val_loss: 878.3682\n",
            "Epoch 129/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 855.2116 - val_loss: 870.4374\n",
            "Epoch 130/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 845.5743 - val_loss: 860.3053\n",
            "Epoch 131/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 814.6629 - val_loss: 855.8312\n",
            "Epoch 132/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 881.8272 - val_loss: 848.3392\n",
            "Epoch 133/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 1095.3110 - val_loss: 849.4339\n",
            "Epoch 134/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 684.0280 - val_loss: 846.1392\n",
            "Epoch 135/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 958.7288 - val_loss: 839.8860\n",
            "Epoch 136/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 812.0900 - val_loss: 821.0822\n",
            "Epoch 137/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 846.7961 - val_loss: 819.2051\n",
            "Epoch 138/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 700.7985 - val_loss: 825.0891\n",
            "Epoch 139/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 771.1463 - val_loss: 808.5684\n",
            "Epoch 140/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 702.3363 - val_loss: 800.0347\n",
            "Epoch 141/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 937.3822 - val_loss: 798.7475\n",
            "Epoch 142/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 724.9427 - val_loss: 794.3891\n",
            "Epoch 143/300\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 726.0834 - val_loss: 783.6068\n",
            "Epoch 144/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 821.5473 - val_loss: 774.1198\n",
            "Epoch 145/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 666.0662 - val_loss: 781.1632\n",
            "Epoch 146/300\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 888.5284 - val_loss: 780.0162\n",
            "Epoch 147/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 675.8037 - val_loss: 764.0919\n",
            "Epoch 148/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 670.6003 - val_loss: 759.4521\n",
            "Epoch 149/300\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 729.2585 - val_loss: 760.7210\n",
            "Epoch 150/300\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 763.2354 - val_loss: 753.9614\n",
            "Epoch 151/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 770.9539 - val_loss: 753.6766\n",
            "Epoch 152/300\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 700.5577 - val_loss: 748.9923\n",
            "Epoch 153/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 755.6866 - val_loss: 733.6815\n",
            "Epoch 154/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 738.1751 - val_loss: 731.6951\n",
            "Epoch 155/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 767.3260 - val_loss: 729.9557\n",
            "Epoch 156/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 685.0973 - val_loss: 735.2615\n",
            "Epoch 157/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 686.2542 - val_loss: 724.3687\n",
            "Epoch 158/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 656.6370 - val_loss: 722.5094\n",
            "Epoch 159/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 623.6352 - val_loss: 715.0938\n",
            "Epoch 160/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 720.1899 - val_loss: 738.8346\n",
            "Epoch 161/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 634.6838 - val_loss: 715.6000\n",
            "Epoch 162/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 748.0341 - val_loss: 704.1536\n",
            "Epoch 163/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 595.5864 - val_loss: 707.2711\n",
            "Epoch 164/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 613.3535 - val_loss: 694.0136\n",
            "Epoch 165/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 645.2720 - val_loss: 692.6389\n",
            "Epoch 166/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 650.2804 - val_loss: 685.1301\n",
            "Epoch 167/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 662.4512 - val_loss: 685.2351\n",
            "Epoch 168/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 666.4652 - val_loss: 678.4545\n",
            "Epoch 169/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 639.6750 - val_loss: 687.5357\n",
            "Epoch 170/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 783.3062 - val_loss: 679.0807\n",
            "Epoch 171/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 683.2053 - val_loss: 673.9128\n",
            "Epoch 172/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 530.4011 - val_loss: 667.8177\n",
            "Epoch 173/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 651.4672 - val_loss: 653.0980\n",
            "Epoch 174/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 532.8567 - val_loss: 649.8107\n",
            "Epoch 175/300\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 642.1730 - val_loss: 648.8522\n",
            "Epoch 176/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 560.4049 - val_loss: 654.1473\n",
            "Epoch 177/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 538.8079 - val_loss: 663.0466\n",
            "Epoch 178/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 723.3393 - val_loss: 639.9855\n",
            "Epoch 179/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 541.7480 - val_loss: 630.1114\n",
            "Epoch 180/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 601.6477 - val_loss: 630.4475\n",
            "Epoch 181/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 603.0497 - val_loss: 635.2019\n",
            "Epoch 182/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 667.2205 - val_loss: 617.0639\n",
            "Epoch 183/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 572.6906 - val_loss: 619.5906\n",
            "Epoch 184/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 681.2632 - val_loss: 611.0469\n",
            "Epoch 185/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 647.6818 - val_loss: 617.8239\n",
            "Epoch 186/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 521.4485 - val_loss: 609.8239\n",
            "Epoch 187/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 587.8805 - val_loss: 617.7590\n",
            "Epoch 188/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 600.7750 - val_loss: 602.0366\n",
            "Epoch 189/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 657.6444 - val_loss: 596.2575\n",
            "Epoch 190/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 581.2368 - val_loss: 599.3684\n",
            "Epoch 191/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 634.3610 - val_loss: 594.8519\n",
            "Epoch 192/300\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 540.3109 - val_loss: 609.2922\n",
            "Epoch 193/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 491.0653 - val_loss: 582.0529\n",
            "Epoch 194/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 520.8981 - val_loss: 571.9339\n",
            "Epoch 195/300\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 630.9314 - val_loss: 578.8177\n",
            "Epoch 196/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 579.9495 - val_loss: 574.3196\n",
            "Epoch 197/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 545.3923 - val_loss: 566.1743\n",
            "Epoch 198/300\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 594.5957 - val_loss: 571.0652\n",
            "Epoch 199/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 567.0495 - val_loss: 571.9934\n",
            "Epoch 200/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 525.2511 - val_loss: 553.6911\n",
            "Epoch 201/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 520.4075 - val_loss: 562.4313\n",
            "Epoch 202/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 621.5143 - val_loss: 555.0708\n",
            "Epoch 203/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 606.0162 - val_loss: 545.7574\n",
            "Epoch 204/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 602.2639 - val_loss: 542.7003\n",
            "Epoch 205/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 478.6383 - val_loss: 543.2942\n",
            "Epoch 206/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 520.3502 - val_loss: 541.5826\n",
            "Epoch 207/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 474.4861 - val_loss: 550.2645\n",
            "Epoch 208/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 498.2488 - val_loss: 531.6310\n",
            "Epoch 209/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 460.2576 - val_loss: 527.4100\n",
            "Epoch 210/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 472.8809 - val_loss: 509.6897\n",
            "Epoch 211/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 447.9111 - val_loss: 515.2460\n",
            "Epoch 212/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 451.6426 - val_loss: 497.5999\n",
            "Epoch 213/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 523.7719 - val_loss: 498.4228\n",
            "Epoch 214/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 587.6707 - val_loss: 490.5265\n",
            "Epoch 215/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 502.9807 - val_loss: 488.5959\n",
            "Epoch 216/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 456.3123 - val_loss: 499.3045\n",
            "Epoch 217/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 449.9688 - val_loss: 490.4266\n",
            "Epoch 218/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 531.4141 - val_loss: 476.5760\n",
            "Epoch 219/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 356.9874 - val_loss: 500.1687\n",
            "Epoch 220/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 502.1361 - val_loss: 464.6021\n",
            "Epoch 221/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 425.3853 - val_loss: 459.2437\n",
            "Epoch 222/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 469.1435 - val_loss: 456.1245\n",
            "Epoch 223/300\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 459.1809 - val_loss: 457.7213\n",
            "Epoch 224/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 429.5331 - val_loss: 446.8603\n",
            "Epoch 225/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 487.3509 - val_loss: 440.9761\n",
            "Epoch 226/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 535.3068 - val_loss: 454.0210\n",
            "Epoch 227/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 348.0650 - val_loss: 431.0170\n",
            "Epoch 228/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 477.8298 - val_loss: 431.7980\n",
            "Epoch 229/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 401.0272 - val_loss: 442.6147\n",
            "Epoch 230/300\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 495.0168 - val_loss: 429.3551\n",
            "Epoch 231/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 407.6879 - val_loss: 429.7060\n",
            "Epoch 232/300\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 427.4123 - val_loss: 421.6161\n",
            "Epoch 233/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 459.4811 - val_loss: 424.3933\n",
            "Epoch 234/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 440.9788 - val_loss: 410.2980\n",
            "Epoch 235/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 442.7098 - val_loss: 408.2018\n",
            "Epoch 236/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 428.5267 - val_loss: 404.3650\n",
            "Epoch 237/300\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 436.6976 - val_loss: 400.9001\n",
            "Epoch 238/300\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 501.1657 - val_loss: 404.9455\n",
            "Epoch 239/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 461.9674 - val_loss: 401.1689\n",
            "Epoch 240/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 337.6543 - val_loss: 392.1004\n",
            "Epoch 241/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 430.8427 - val_loss: 396.2810\n",
            "Epoch 242/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 360.7964 - val_loss: 390.3684\n",
            "Epoch 243/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 420.3867 - val_loss: 385.4256\n",
            "Epoch 244/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 440.7724 - val_loss: 385.8944\n",
            "Epoch 245/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 400.3576 - val_loss: 386.9593\n",
            "Epoch 246/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 326.6550 - val_loss: 381.4731\n",
            "Epoch 247/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 450.0780 - val_loss: 383.4807\n",
            "Epoch 248/300\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 389.4342 - val_loss: 368.1860\n",
            "Epoch 249/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 407.2429 - val_loss: 374.1607\n",
            "Epoch 250/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 406.7234 - val_loss: 366.0193\n",
            "Epoch 251/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 374.7684 - val_loss: 382.3030\n",
            "Epoch 252/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 388.2029 - val_loss: 363.3292\n",
            "Epoch 253/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 440.4690 - val_loss: 358.3278\n",
            "Epoch 254/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 327.2662 - val_loss: 352.7305\n",
            "Epoch 255/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 374.6731 - val_loss: 353.6055\n",
            "Epoch 256/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 412.8239 - val_loss: 348.4927\n",
            "Epoch 257/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 341.3047 - val_loss: 358.0067\n",
            "Epoch 258/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 293.4179 - val_loss: 359.0134\n",
            "Epoch 259/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 293.5245 - val_loss: 340.5010\n",
            "Epoch 260/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 288.0753 - val_loss: 343.9630\n",
            "Epoch 261/300\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 414.6670 - val_loss: 347.3029\n",
            "Epoch 262/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 300.9933 - val_loss: 329.3658\n",
            "Epoch 263/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 343.4391 - val_loss: 333.8688\n",
            "Epoch 264/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 265.7946 - val_loss: 331.9807\n",
            "Epoch 265/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 358.3664 - val_loss: 333.7912\n",
            "Epoch 266/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 314.4789 - val_loss: 317.7025\n",
            "Epoch 267/300\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 326.2914 - val_loss: 318.2845\n",
            "Epoch 268/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 346.5064 - val_loss: 322.5506\n",
            "Epoch 269/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 243.7184 - val_loss: 313.4795\n",
            "Epoch 270/300\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 409.2597 - val_loss: 316.6433\n",
            "Epoch 271/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 273.8449 - val_loss: 305.6819\n",
            "Epoch 272/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 314.7753 - val_loss: 304.7369\n",
            "Epoch 273/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 352.6723 - val_loss: 305.8323\n",
            "Epoch 274/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 247.7672 - val_loss: 305.2301\n",
            "Epoch 275/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 344.5502 - val_loss: 307.5012\n",
            "Epoch 276/300\n",
            "43/43 [==============================] - 0s 3ms/step - loss: 324.9360 - val_loss: 305.3485\n",
            "Epoch 277/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 261.9694 - val_loss: 294.8636\n",
            "Epoch 278/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 329.6255 - val_loss: 294.3979\n",
            "Epoch 279/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 277.4050 - val_loss: 287.7166\n",
            "Epoch 280/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 279.8672 - val_loss: 286.0312\n",
            "Epoch 281/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 269.9170 - val_loss: 277.8812\n",
            "Epoch 282/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 300.9606 - val_loss: 287.7822\n",
            "Epoch 283/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 303.8669 - val_loss: 278.2693\n",
            "Epoch 284/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 280.0815 - val_loss: 272.7532\n",
            "Epoch 285/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 261.2946 - val_loss: 274.1167\n",
            "Epoch 286/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 280.7340 - val_loss: 269.4163\n",
            "Epoch 287/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 229.4771 - val_loss: 269.4059\n",
            "Epoch 288/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 254.0881 - val_loss: 265.7867\n",
            "Epoch 289/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 302.7819 - val_loss: 262.5286\n",
            "Epoch 290/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 242.0969 - val_loss: 260.0423\n",
            "Epoch 291/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 239.2611 - val_loss: 254.5292\n",
            "Epoch 292/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 233.3909 - val_loss: 260.2043\n",
            "Epoch 293/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 255.4415 - val_loss: 250.5967\n",
            "Epoch 294/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 253.0293 - val_loss: 252.5537\n",
            "Epoch 295/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 241.9062 - val_loss: 250.9696\n",
            "Epoch 296/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 242.4661 - val_loss: 246.5620\n",
            "Epoch 297/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 206.7602 - val_loss: 241.4729\n",
            "Epoch 298/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 250.8570 - val_loss: 243.4491\n",
            "Epoch 299/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 207.6844 - val_loss: 234.3795\n",
            "Epoch 300/300\n",
            "43/43 [==============================] - 0s 2ms/step - loss: 221.0966 - val_loss: 240.6521\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "kesUrwoV3Cfw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b25e233f-182d-4ee1-b4cd-cce722c93b4c"
      },
      "source": [
        "X_test.shape"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(458, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s96l2G253Cfx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02bca751-8d92-4ea4-cfaf-9fb6e1df19c2"
      },
      "source": [
        "model.predict(X_test[:10])"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 474.4767 ,  250.25938, 1255.8457 ,  880.4093 , 1194.2603 ,\n",
              "         789.68585],\n",
              "       [ 537.6549 ,  239.47374, 1250.9512 ,  878.29474, 1041.4608 ,\n",
              "         522.6125 ],\n",
              "       [ 165.53528,  183.99875, 1520.8156 , 1062.1396 , 1449.7542 ,\n",
              "         993.86725],\n",
              "       [ 483.69864,  287.58405, 1421.8846 ,  992.1325 , 1276.8752 ,\n",
              "         830.7962 ],\n",
              "       [ 433.72726,  182.37372, 1352.6035 ,  949.24695,  942.8351 ,\n",
              "         209.50462],\n",
              "       [ 642.9681 ,  346.68356, 1733.6257 , 1209.1189 , 1149.208  ,\n",
              "         268.7492 ],\n",
              "       [ 115.99806,  128.44533, 1270.2354 ,  876.4655 ,  977.0773 ,\n",
              "         338.43018],\n",
              "       [ 365.66495,  133.72269, 1098.169  ,  770.7686 ,  820.1865 ,\n",
              "         256.9381 ],\n",
              "       [ 482.1333 ,  236.1432 , 1358.761  ,  951.2519 , 1051.8815 ,\n",
              "         506.04102],\n",
              "       [ 289.95813,  192.91263, 1486.5809 , 1041.9633 , 1039.3801 ,\n",
              "         480.27142]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "ZmfB8q2g3Cfx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98b0206d-11a0-45d7-8921-e0cc4de4fb55"
      },
      "source": [
        "y_test.shape"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(458, 6)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISdK2dn13Cfx"
      },
      "source": [
        "import pandas as pd\n",
        "y_pred = model.predict(X_test)     # ravel 삭제"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3iDsLirV3Cfy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f2edff7-50a9-47af-fc73-ec4b02003a21"
      },
      "source": [
        "y_pred.shape"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(458, 6)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TycQb8Ix3Cfy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e155f53-95d1-44aa-8d7c-ec13e4b2e6f6"
      },
      "source": [
        "y_test.shape"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(458, 6)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqK8A75B4Nbo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3384ab3-0479-4c5b-ae03-2a1397a62065"
      },
      "source": [
        "from sklearn.metrics import r2_score\n",
        "r2_score(y_test, y_pred)     # 학습 부족    # 학습 과대 "
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9936764079637888"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20LZh3WbALnF",
        "outputId": "f1e0c537-b3be-4d1c-d362-a8a424ecaeaa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#model.get_weights()\n",
        "\n",
        "#model.get_weights()[2].shape\n",
        "\n",
        "weights = model.get_weights()\n",
        "weights\n",
        "#model.set_weights(weights)\n",
        "\n",
        "print(weights[0].shape)\n",
        "print(weights[1].shape)\n",
        "print(weights[2].shape)\n",
        "print(weights[3].shape)\n",
        "print(weights[4].shape)\n",
        "print(weights[5].shape)\n"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4, 70)\n",
            "(70,)\n",
            "(70, 70)\n",
            "(70,)\n",
            "(70, 6)\n",
            "(6,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qlDfza0Nnsr"
      },
      "source": [
        "np.savetxt('weights[0].csv', weights[0])\n",
        "np.savetxt('weights[1].csv', weights[1])\n",
        "np.savetxt('weights[2].csv', weights[2])\n",
        "np.savetxt('weights[3].csv', weights[3])\n",
        "np.savetxt('weights[4].csv', weights[4])\n",
        "np.savetxt('weights[5].csv', weights[5])\n"
      ],
      "execution_count": 105,
      "outputs": []
    }
  ]
}